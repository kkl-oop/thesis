{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ljj12\\Temp\\jieba.cache\n",
      "Loading model cost 2.194 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> 完成对第100章.txt内容的分词\n",
      "=> 完成对第101章.txt内容的分词\n",
      "=> 完成对第102章.txt内容的分词\n",
      "=> 完成对第103章.txt内容的分词\n",
      "=> 完成对第104章.txt内容的分词\n",
      "=> 完成对第105章.txt内容的分词\n",
      "=> 完成对第106章.txt内容的分词\n",
      "=> 完成对第107章.txt内容的分词\n",
      "=> 完成对第108章.txt内容的分词\n",
      "=> 完成对第109章.txt内容的分词\n",
      "=> 完成对第10章.txt内容的分词\n",
      "=> 完成对第110章.txt内容的分词\n",
      "=> 完成对第111章.txt内容的分词\n",
      "=> 完成对第112章.txt内容的分词\n",
      "=> 完成对第113章.txt内容的分词\n",
      "=> 完成对第114章.txt内容的分词\n",
      "=> 完成对第115章.txt内容的分词\n",
      "=> 完成对第116章.txt内容的分词\n",
      "=> 完成对第117章.txt内容的分词\n",
      "=> 完成对第118章.txt内容的分词\n",
      "=> 完成对第119章.txt内容的分词\n",
      "=> 完成对第11章.txt内容的分词\n",
      "=> 完成对第120章.txt内容的分词\n",
      "=> 完成对第121章.txt内容的分词\n",
      "=> 完成对第122章.txt内容的分词\n",
      "=> 完成对第123章.txt内容的分词\n",
      "=> 完成对第124章.txt内容的分词\n",
      "=> 完成对第125章.txt内容的分词\n",
      "=> 完成对第126章.txt内容的分词\n",
      "=> 完成对第127章.txt内容的分词\n",
      "=> 完成对第128章.txt内容的分词\n",
      "=> 完成对第129章.txt内容的分词\n",
      "=> 完成对第12章.txt内容的分词\n",
      "=> 完成对第130章.txt内容的分词\n",
      "=> 完成对第131章.txt内容的分词\n",
      "=> 完成对第132章.txt内容的分词\n",
      "=> 完成对第133章.txt内容的分词\n",
      "=> 完成对第134章.txt内容的分词\n",
      "=> 完成对第135章.txt内容的分词\n",
      "=> 完成对第136章.txt内容的分词\n",
      "=> 完成对第137章.txt内容的分词\n",
      "=> 完成对第138章.txt内容的分词\n",
      "=> 完成对第139章.txt内容的分词\n",
      "=> 完成对第13章.txt内容的分词\n",
      "=> 完成对第140章.txt内容的分词\n",
      "=> 完成对第141章.txt内容的分词\n",
      "=> 完成对第142章.txt内容的分词\n",
      "=> 完成对第143章.txt内容的分词\n",
      "=> 完成对第144章.txt内容的分词\n",
      "=> 完成对第145章.txt内容的分词\n",
      "=> 完成对第146章.txt内容的分词\n",
      "=> 完成对第147章.txt内容的分词\n",
      "=> 完成对第148章.txt内容的分词\n",
      "=> 完成对第149章.txt内容的分词\n",
      "=> 完成对第14章.txt内容的分词\n",
      "=> 完成对第150章.txt内容的分词\n",
      "=> 完成对第151章.txt内容的分词\n",
      "=> 完成对第152章.txt内容的分词\n",
      "=> 完成对第153章.txt内容的分词\n",
      "=> 完成对第154章.txt内容的分词\n",
      "=> 完成对第155章.txt内容的分词\n",
      "=> 完成对第156章.txt内容的分词\n",
      "=> 完成对第157章.txt内容的分词\n",
      "=> 完成对第158章.txt内容的分词\n",
      "=> 完成对第159章.txt内容的分词\n",
      "=> 完成对第15章.txt内容的分词\n",
      "=> 完成对第160章.txt内容的分词\n",
      "=> 完成对第161章.txt内容的分词\n",
      "=> 完成对第162章.txt内容的分词\n",
      "=> 完成对第16章.txt内容的分词\n",
      "=> 完成对第17章.txt内容的分词\n",
      "=> 完成对第18章.txt内容的分词\n",
      "=> 完成对第19章.txt内容的分词\n",
      "=> 完成对第1章.txt内容的分词\n",
      "=> 完成对第20章.txt内容的分词\n",
      "=> 完成对第21章.txt内容的分词\n",
      "=> 完成对第22章.txt内容的分词\n",
      "=> 完成对第23章.txt内容的分词\n",
      "=> 完成对第24章.txt内容的分词\n",
      "=> 完成对第25章.txt内容的分词\n",
      "=> 完成对第26章.txt内容的分词\n",
      "=> 完成对第27章.txt内容的分词\n",
      "=> 完成对第28章.txt内容的分词\n",
      "=> 完成对第29章.txt内容的分词\n",
      "=> 完成对第2章.txt内容的分词\n",
      "=> 完成对第30章.txt内容的分词\n",
      "=> 完成对第31章.txt内容的分词\n",
      "=> 完成对第32章.txt内容的分词\n",
      "=> 完成对第33章.txt内容的分词\n",
      "=> 完成对第34章.txt内容的分词\n",
      "=> 完成对第35章.txt内容的分词\n",
      "=> 完成对第36章.txt内容的分词\n",
      "=> 完成对第37章.txt内容的分词\n",
      "=> 完成对第38章.txt内容的分词\n",
      "=> 完成对第39章.txt内容的分词\n",
      "=> 完成对第3章.txt内容的分词\n",
      "=> 完成对第40章.txt内容的分词\n",
      "=> 完成对第41章.txt内容的分词\n",
      "=> 完成对第42章.txt内容的分词\n",
      "=> 完成对第43章.txt内容的分词\n",
      "=> 完成对第44章.txt内容的分词\n",
      "=> 完成对第45章.txt内容的分词\n",
      "=> 完成对第46章.txt内容的分词\n",
      "=> 完成对第47章.txt内容的分词\n",
      "=> 完成对第48章.txt内容的分词\n",
      "=> 完成对第49章.txt内容的分词\n",
      "=> 完成对第4章.txt内容的分词\n",
      "=> 完成对第50章.txt内容的分词\n",
      "=> 完成对第51章.txt内容的分词\n",
      "=> 完成对第52章.txt内容的分词\n",
      "=> 完成对第53章.txt内容的分词\n",
      "=> 完成对第54章.txt内容的分词\n",
      "=> 完成对第55章.txt内容的分词\n",
      "=> 完成对第56章.txt内容的分词\n",
      "=> 完成对第57章.txt内容的分词\n",
      "=> 完成对第58章.txt内容的分词\n",
      "=> 完成对第59章.txt内容的分词\n",
      "=> 完成对第5章.txt内容的分词\n",
      "=> 完成对第60章.txt内容的分词\n",
      "=> 完成对第61章.txt内容的分词\n",
      "=> 完成对第62章.txt内容的分词\n",
      "=> 完成对第63章.txt内容的分词\n",
      "=> 完成对第64章.txt内容的分词\n",
      "=> 完成对第65章.txt内容的分词\n",
      "=> 完成对第66章.txt内容的分词\n",
      "=> 完成对第67章.txt内容的分词\n",
      "=> 完成对第68章.txt内容的分词\n",
      "=> 完成对第69章.txt内容的分词\n",
      "=> 完成对第6章.txt内容的分词\n",
      "=> 完成对第70章.txt内容的分词\n",
      "=> 完成对第71章.txt内容的分词\n",
      "=> 完成对第72章.txt内容的分词\n",
      "=> 完成对第73章.txt内容的分词\n",
      "=> 完成对第74章.txt内容的分词\n",
      "=> 完成对第75章.txt内容的分词\n",
      "=> 完成对第76章.txt内容的分词\n",
      "=> 完成对第77章.txt内容的分词\n",
      "=> 完成对第78章.txt内容的分词\n",
      "=> 完成对第79章.txt内容的分词\n",
      "=> 完成对第7章.txt内容的分词\n",
      "=> 完成对第80章.txt内容的分词\n",
      "=> 完成对第81章.txt内容的分词\n",
      "=> 完成对第82章.txt内容的分词\n",
      "=> 完成对第83章.txt内容的分词\n",
      "=> 完成对第84章.txt内容的分词\n",
      "=> 完成对第85章.txt内容的分词\n",
      "=> 完成对第86章.txt内容的分词\n",
      "=> 完成对第87章.txt内容的分词\n",
      "=> 完成对第88章.txt内容的分词\n",
      "=> 完成对第89章.txt内容的分词\n",
      "=> 完成对第8章.txt内容的分词\n",
      "=> 完成对第90章.txt内容的分词\n",
      "=> 完成对第91章.txt内容的分词\n",
      "=> 完成对第92章.txt内容的分词\n",
      "=> 完成对第93章.txt内容的分词\n",
      "=> 完成对第94章.txt内容的分词\n",
      "=> 完成对第95章.txt内容的分词\n",
      "=> 完成对第96章.txt内容的分词\n",
      "=> 完成对第97章.txt内容的分词\n",
      "=> 完成对第98章.txt内容的分词\n",
      "=> 完成对第99章.txt内容的分词\n",
      "=> 完成对第9章.txt内容的分词\n",
      "=> 分词完成\n"
     ]
    }
   ],
   "source": [
    "import jieba, os, re\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "def stopwordslist():\n",
    "  stopwords = [line.strip() for line in open('./stopwords.txt', encoding='utf-8').readlines()]\n",
    "  return stopwords\n",
    "\n",
    "# stopwordslist()\n",
    "\n",
    "\n",
    "def seg_depart(sentence):\n",
    "  '''\n",
    "  切分单个句子成分词, sentence输的句子\n",
    "  '''\n",
    "  jieba.load_userdict(\"../socialnet/name.txt\")\n",
    "  sentence_depart = jieba.cut(sentence.strip())\n",
    "  stopwords = stopwordslist()\n",
    "  outstr = ''\n",
    "  \n",
    "  for word in sentence_depart:\n",
    "    if word not in stopwords:\n",
    "      outstr += word\n",
    "      outstr += \" \"\n",
    "      \n",
    "  return outstr\n",
    "\n",
    "\n",
    "def generate_chapeters_from_book(file):\n",
    "  '''\n",
    "  从整本书中生成每一章\n",
    "  '''\n",
    "  chapter = \"\"\n",
    "  lines = file.readlines()\n",
    "  chapter_token = u\"第[\\u4E00-\\u9FA5]+章\"\n",
    "  pattern = re.compile(chapter_token)\n",
    "  num = 0 #章节数\n",
    "  if not os.path.exists(\"./book\"):\n",
    "    os.makedirs(\"./book\")\n",
    "  for line in lines:\n",
    "    if pattern.search(line) is None:\n",
    "      chapter += line\n",
    "    else:\n",
    "      if num == 0:\n",
    "        num += 1\n",
    "        continue\n",
    "      with open(\"./book/第{}章.txt\".format(num),mode='w',encoding='utf-8') as f:\n",
    "        f.write(chapter)\n",
    "        \n",
    "      chapter = \"\"\n",
    "      num += 1\n",
    "      \n",
    "  with open(\"./book/第{}章.txt\".format(num),mode='w',encoding='utf-8') as f:\n",
    "    f.write(chapter)\n",
    "\n",
    "\n",
    "with open(\"./book.txt\",encoding='utf-8') as file:\n",
    "  generate_chapeters_from_book(file)\n",
    "    \n",
    "    \n",
    "def tokenize_chapter(chapter, chapter_name):\n",
    "  '''\n",
    "  对每个章节进行分词, 并保存到对应的文件中\n",
    "  '''\n",
    "  output_file = \"分词_\" + chapter_name\n",
    "  if not os.path.exists(\"./分词\"):\n",
    "    os.makedirs(\"./分词\")\n",
    "  with open(\"./分词/\"+output_file,mode='w',encoding='utf-8') as f:\n",
    "    for line in chapter:\n",
    "      line = re.sub(u'[^\\u4e00-\\u9fa5]+','',line)\n",
    "      line_seg = seg_depart(line.strip())\n",
    "      f.write(line_seg.strip()+'\\n')\n",
    "  \n",
    "  print(\"=> 完成对{}内容的分词\".format(chapter_name))\n",
    "\n",
    "def tokenize():\n",
    "  file_names = list(os.listdir(\"./book\"))\n",
    "  for file_name in file_names:\n",
    "    with open(\"./book/\"+file_name, mode='r', encoding='utf-8') as chapter:\n",
    "      tokenize_chapter(chapter.readlines(), file_name)\n",
    "\n",
    "  print(\"=> 分词完成\")\n",
    "\n",
    "tokenize()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10个主题的单词分布为：\n",
      "\n",
      "(0, '0.012*\"李向前\" + 0.012*\"手术\" + 0.008*\"车\" + 0.008*\"黄原\" + 0.008*\"腿\" + 0.008*\"那位\" + 0.008*\"弟弟\" + 0.008*\"护士\" + 0.008*\"李登云\" + 0.008*\"想到\"')\n",
      "(1, '0.014*\"一盘\" + 0.014*\"两\" + 0.014*\"老板\" + 0.011*\"说\" + 0.011*\"润生\" + 0.009*\"烧酒\" + 0.008*\"开车\" + 0.008*\"招呼\" + 0.008*\"耳朵\" + 0.008*\"凉拌\"')\n",
      "(2, '0.021*\"值班\" + 0.016*\"司机\" + 0.016*\"门房\" + 0.016*\"急诊室\" + 0.011*\"向前\" + 0.011*\"外科\" + 0.011*\"护士\" + 0.011*\"下车\" + 0.011*\"老头\" + 0.011*\"跳\"')\n",
      "(3, '0.022*\"医院\" + 0.013*\"地区\" + 0.011*\"这位\" + 0.010*\"司机\" + 0.010*\"面包车\" + 0.009*\"昏迷\" + 0.008*\"书记\" + 0.008*\"李登云\" + 0.008*\"卫生局\" + 0.007*\"李向前\"')\n",
      "(4, '0.019*\"向前\" + 0.013*\"脸色\" + 0.012*\"汽车\" + 0.009*\"一半\" + 0.009*\"情况\" + 0.008*\"帮子\" + 0.008*\"喊\" + 0.008*\"惊叫\" + 0.008*\"难兄难弟\" + 0.008*\"停\"')\n",
      "(5, '0.019*\"李向前\" + 0.015*\"汽车\" + 0.012*\"这位\" + 0.012*\"公司\" + 0.011*\"说\" + 0.009*\"运输\" + 0.009*\"原西县\" + 0.009*\"师傅\" + 0.009*\"里\" + 0.008*\"名字\"')\n",
      "(6, '0.017*\"出车\" + 0.013*\"李向前\" + 0.012*\"里\" + 0.011*\"润生\" + 0.010*\"时间\" + 0.010*\"驾驶\" + 0.008*\"技术\" + 0.008*\"熟练\" + 0.008*\"单独\" + 0.008*\"让润生\"')\n",
      "(7, '0.015*\"李登云\" + 0.015*\"一位\" + 0.015*\"手术室\" + 0.012*\"李向前\" + 0.010*\"胳膊\" + 0.010*\"门\" + 0.010*\"医院\" + 0.010*\"干事\" + 0.010*\"关闭\" + 0.010*\"护士\"')\n",
      "(8, '0.011*\"里\" + 0.010*\"膝盖\" + 0.009*\"向前\" + 0.009*\"两个\" + 0.008*\"住\" + 0.008*\"痛苦\" + 0.008*\"先\" + 0.008*\"跑\" + 0.007*\"老人\" + 0.007*\"赶紧\"')\n",
      "(9, '0.012*\"院长\" + 0.009*\"妻弟\" + 0.008*\"主任医师\" + 0.008*\"正副\" + 0.008*\"局长\" + 0.008*\"医院\" + 0.007*\"说\" + 0.007*\"等待\" + 0.007*\"永远\" + 0.007*\"两个\"')\n"
     ]
    }
   ],
   "source": [
    "dir = \"./分词/\"\n",
    "tokenized_filenames = os.listdir(dir)\n",
    "\n",
    "with open(dir+tokenized_filenames[0],mode='r',encoding='utf-8') as tokenized_chapter:\n",
    "  tokenized_lines = []\n",
    "  for line in tokenized_chapter:\n",
    "    line = [word.strip() for word in line.split(' ')]\n",
    "    tokenized_lines.append(line)\n",
    "    \n",
    "  # print(tokenized_line) \n",
    "  '''构建词频矩阵，训练LDA模型'''\n",
    "  dictionary = corpora.Dictionary(tokenized_lines) \n",
    "  corpus = [dictionary.doc2bow(text) for text in tokenized_lines]\n",
    "  \n",
    "  lda = models.LdaModel(corpus=corpus,id2word=dictionary,num_topics=10)\n",
    "  topic_list = lda.print_topics(10)\n",
    "  print(\"10个主题的单词分布为：\\n\")\n",
    "  for topic in topic_list:\n",
    "    print(topic)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5085d9696cf3b63ae5bb775278ec3e393a5cd86897776366bf95a1bc2610bf80"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
