{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\u000b\f\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import string\n",
    "import random\n",
    "import sys\n",
    "import unidecode\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "print(all_characters)\n",
    "file = unidecode.unidecode(open('./data/names.txt').read())\n",
    "# print(file)\n",
    "class RNN(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "    super(RNN, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.embed = nn.Embedding(input_size, hidden_size)\n",
    "    self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "    self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "  def forward(self,x,hidden,cell):\n",
    "    out = self.embed(x)\n",
    "    out, (hidden,cell) = self.lstm(out.unsqueeze(1),(hidden,cell))\n",
    "    out = self.fc(out.reshape(out.shape[0],-1))\n",
    "    return out, (hidden,cell)\n",
    "  \n",
    "  def init_hidden(self,batch_size):\n",
    "    hidden = torch.zeros(self.num_layers,batch_size,self.hidden_size).to(device)\n",
    "    cell = torch.zeros(self.num_layers,batch_size,self.hidden_size).to(device)\n",
    "    return hidden,cell\n",
    "\n",
    "class Generator():\n",
    "  def __init__(self):\n",
    "    self.chunk_len = 250\n",
    "    self.num_epochs = 5000\n",
    "    self.batch_size = 1\n",
    "    self.batch_size = 1\n",
    "    self.print_every = 50\n",
    "    self.hidden_size = 256\n",
    "    self.num_layers = 2\n",
    "    self.lr = 0.003\n",
    "    \n",
    "  def char_tensor(self, string):\n",
    "    tensor = torch.zeros(len(string).long())\n",
    "    for c in range(len(string)):\n",
    "      tensor[c] = all_characters.index(string[c])\n",
    "      \n",
    "    return tensor\n",
    "  \n",
    "  def get_random_batch(self):\n",
    "    start_idx = random.randint(0,len(file) - self.chunk_len)\n",
    "    end_idx = start_idx + self.chunk_len + 1\n",
    "    text_str = file[start_idx + self.chunk_len + 1]\n",
    "    text_input = torch.zeros(self.batch_size,self.chunk_len)\n",
    "    text_target = torch.zeros(self.batch_size, self.chunk_len)\n",
    "    \n",
    "    for i in range(self.batch_size):\n",
    "      text_input[i,:] = self.char_tensor(text_str[:-1])\n",
    "      text_target[i,:] = self.char_tensor(text_str[1:])    \n",
    "    \n",
    "    return text_input.long(), text_target.long()\n",
    "    \n",
    "  \n",
    "  def generator(self, initial_str='Ab', predict_len=100,temperature=0.85):\n",
    "    hidden, cell = self.rnn.init_hidden(batch_size=self.batch_szie)\n",
    "    initial_input = self.char_tensor(initial_str)\n",
    "    predicted = initial_str\n",
    "    \n",
    "    for p in range(len(initial_str) - 1):\n",
    "      _, (hidden,cell) = self.rnn(initial_input[p].view(1).to(device),hidden,cell)\n",
    "      \n",
    "    last_char = initial_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "      output, (hidden, cell) = self.rnn(initial_input[p].view(1).to(device),hidden,cell)\n",
    "      output_dist = output.data.view(-1).div(temperature)()\n",
    "  \n",
    "  def train(self):\n",
    "    self.rnn = RNN(n_characters, self.hidden_size, self.num_layers, n_characters)\n",
    "    optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    writer = SummaryWriter(f'runs/names0')\n",
    "    \n",
    "    print(\"=> Starting training\")\n",
    "    for epoch in range(1, self.num_epochs + 1):\n",
    "      inp, target = get_random_batch()\n",
    "      hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n",
    "      \n",
    "      self.rnn.zero_grad()\n",
    "      loss = 0\n",
    "      inp = inp.to(device)\n",
    "      target = target.to(device)\n",
    "      \n",
    "      for c in range(self.chunk_len):\n",
    "        output, (hidden, cell) = self.rnn(inp[:,c],hidden,cell)\n",
    "        loss += criterion(output,target[:,c])\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      loss = loss.item() / self.hidden_size\n",
    "\n",
    "    if epoch % self.print_every == 0:\n",
    "      print(f\"Loss: {loss}\")\n",
    "      \n",
    "    writer.add_scalar(\"Training loss\", loss, global_step=epoch)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5085d9696cf3b63ae5bb775278ec3e393a5cd86897776366bf95a1bc2610bf80"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
